{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Loan Approval Prediction - End-to-End Workflow\n",
        "\n",
        "**üéØ Goal**: We will apply a Supervised Learning workflow on the Loan Approval dataset to predict whether a loan application will be approved or not.\n",
        "\n",
        "**üóíÔ∏è Scenario**\n",
        "\n",
        "The Loan Approval dataset contains information about loan applicants, including their personal details, financial information, and loan characteristics. Our task is to build a model that can predict loan approval status.\n",
        "\n",
        "**‚ö° Task**\n",
        "\n",
        "## 1. Imports and Data Loading\n",
        "\n",
        "First, we import the necessary libraries and load the dataset from the provided URL. We then inspect the dataframe to understand its structure and check for missing values, which determines our preprocessing strategy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(614, 13)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load the dataset from the provided URL\n",
        "loan_data = pd.read_csv('https://raw.githubusercontent.com/prasertcbs/basic-dataset/refs/heads/master/Loan-Approval-Prediction.csv')\n",
        "\n",
        "# Display the shape of the dataset\n",
        "loan_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Loan_ID</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Married</th>\n",
              "      <th>Dependents</th>\n",
              "      <th>Education</th>\n",
              "      <th>Self_Employed</th>\n",
              "      <th>ApplicantIncome</th>\n",
              "      <th>CoapplicantIncome</th>\n",
              "      <th>LoanAmount</th>\n",
              "      <th>Loan_Amount_Term</th>\n",
              "      <th>Credit_History</th>\n",
              "      <th>Property_Area</th>\n",
              "      <th>Loan_Status</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LP001002</td>\n",
              "      <td>Male</td>\n",
              "      <td>No</td>\n",
              "      <td>0</td>\n",
              "      <td>Graduate</td>\n",
              "      <td>No</td>\n",
              "      <td>5849</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>360.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Urban</td>\n",
              "      <td>Y</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>LP001003</td>\n",
              "      <td>Male</td>\n",
              "      <td>Yes</td>\n",
              "      <td>1</td>\n",
              "      <td>Graduate</td>\n",
              "      <td>No</td>\n",
              "      <td>4583</td>\n",
              "      <td>1508.0</td>\n",
              "      <td>128.0</td>\n",
              "      <td>360.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Rural</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>LP001005</td>\n",
              "      <td>Male</td>\n",
              "      <td>Yes</td>\n",
              "      <td>0</td>\n",
              "      <td>Graduate</td>\n",
              "      <td>Yes</td>\n",
              "      <td>3000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>66.0</td>\n",
              "      <td>360.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Urban</td>\n",
              "      <td>Y</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>LP001006</td>\n",
              "      <td>Male</td>\n",
              "      <td>Yes</td>\n",
              "      <td>0</td>\n",
              "      <td>Not Graduate</td>\n",
              "      <td>No</td>\n",
              "      <td>2583</td>\n",
              "      <td>2358.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>360.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Urban</td>\n",
              "      <td>Y</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>LP001008</td>\n",
              "      <td>Male</td>\n",
              "      <td>No</td>\n",
              "      <td>0</td>\n",
              "      <td>Graduate</td>\n",
              "      <td>No</td>\n",
              "      <td>6000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>141.0</td>\n",
              "      <td>360.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Urban</td>\n",
              "      <td>Y</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Loan_ID Gender Married Dependents     Education Self_Employed  \\\n",
              "0  LP001002   Male      No          0      Graduate            No   \n",
              "1  LP001003   Male     Yes          1      Graduate            No   \n",
              "2  LP001005   Male     Yes          0      Graduate           Yes   \n",
              "3  LP001006   Male     Yes          0  Not Graduate            No   \n",
              "4  LP001008   Male      No          0      Graduate            No   \n",
              "\n",
              "   ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  \\\n",
              "0             5849                0.0         NaN             360.0   \n",
              "1             4583             1508.0       128.0             360.0   \n",
              "2             3000                0.0        66.0             360.0   \n",
              "3             2583             2358.0       120.0             360.0   \n",
              "4             6000                0.0       141.0             360.0   \n",
              "\n",
              "   Credit_History Property_Area Loan_Status  \n",
              "0             1.0         Urban           Y  \n",
              "1             1.0         Rural           N  \n",
              "2             1.0         Urban           Y  \n",
              "3             1.0         Urban           Y  \n",
              "4             1.0         Urban           Y  "
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Display the first 5 rows to inspect data types and example values\n",
        "loan_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Loan_Status\n",
              "Y    422\n",
              "N    192\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check the distribution of the target variable to see if classes are balanced\n",
        "loan_data['Loan_Status'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Loan_ID               0\n",
              "Gender               13\n",
              "Married               3\n",
              "Dependents           15\n",
              "Education             0\n",
              "Self_Employed        32\n",
              "ApplicantIncome       0\n",
              "CoapplicantIncome     0\n",
              "LoanAmount           22\n",
              "Loan_Amount_Term     14\n",
              "Credit_History       50\n",
              "Property_Area         0\n",
              "Loan_Status           0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Identify columns with missing values to decide on imputation strategies\n",
        "loan_data.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Feature Selection and Splitting\n",
        "\n",
        "We separate the data into the feature matrix (X) and the target vector (y) as specified in the assignment. We will identify numerical and categorical features to build appropriate preprocessing pipelines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature columns:\n",
            "['Loan_ID', 'Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term', 'Credit_History', 'Property_Area']\n",
            "\n",
            "Data types:\n",
            "Loan_ID               object\n",
            "Gender                object\n",
            "Married               object\n",
            "Dependents            object\n",
            "Education             object\n",
            "Self_Employed         object\n",
            "ApplicantIncome        int64\n",
            "CoapplicantIncome    float64\n",
            "LoanAmount           float64\n",
            "Loan_Amount_Term     float64\n",
            "Credit_History       float64\n",
            "Property_Area         object\n",
            "dtype: object\n",
            "\n",
            "Target variable encoding: {'N': 0, 'Y': 1}\n",
            "Target distribution after encoding:\n",
            "1    422\n",
            "0    192\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Split features and target as specified in assignment.md\n",
        "X = loan_data.drop('Loan_Status', axis=1)\n",
        "y = loan_data['Loan_Status']\n",
        "\n",
        "# Display feature names and data types\n",
        "print(\"Feature columns:\")\n",
        "print(X.columns.tolist())\n",
        "print(\"\\nData types:\")\n",
        "print(X.dtypes)\n",
        "\n",
        "# Encode target variable from 'Y'/'N' to 1/0 for sklearn compatibility\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "\n",
        "print(f\"\\nTarget variable encoding: {dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))}\")\n",
        "print(f\"Target distribution after encoding:\\n{pd.Series(y).value_counts()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Numerical features: ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term', 'Credit_History']\n",
            "Categorical features: ['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Property_Area']\n"
          ]
        }
      ],
      "source": [
        "# Identify numerical and categorical features\n",
        "# Numerical features are typically int64 or float64\n",
        "# Categorical features are typically object type\n",
        "# Note: Exclude Loan_ID as it's just an identifier and not useful for prediction\n",
        "\n",
        "numerical_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
        "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "# Remove Loan_ID from categorical features if present\n",
        "if 'Loan_ID' in categorical_features:\n",
        "    categorical_features.remove('Loan_ID')\n",
        "\n",
        "print(\"Numerical features:\", numerical_features)\n",
        "print(\"Categorical features:\", categorical_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Building the Pre-Processing Pipeline\n",
        "\n",
        "This is the core of the workflow. We use Pipeline to chain sequential steps (like imputation and scaling) and ColumnTransformer to apply these different pipelines to specific columns (numerical vs. categorical) simultaneously.\n",
        "\n",
        "- **Numerical Data**: We fill missing values with the median and scale data to unit variance using StandardScaler.\n",
        "\n",
        "- **Categorical Data**: We fill missing values with the most frequent value and convert text categories into binary vectors (One-Hot Encoding).\n",
        "\n",
        "`sklearn`'s pipeline is a tool that allows us to assemble several steps together. It sequentially applies a list of transforms and a final estimator. Intermediate steps of the pipeline must be 'transforms', that is, they must implement fit and transform methods. The final estimator only needs to implement fit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Define pipeline for numerical features\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')), # Fill missing values with median\n",
        "    ('scaler', StandardScaler())                   # Standardize features (mean=0, variance=1)\n",
        "])\n",
        "\n",
        "# 2. Define pipeline for categorical features\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')), # Fill missing with mode\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))    # Convert categories to binary vectors\n",
        "])\n",
        "\n",
        "# 3. Combine transformers using ColumnTransformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        # Apply numerical pipeline to numerical columns\n",
        "        ('num', numerical_transformer, numerical_features),\n",
        "        # Apply categorical pipeline to categorical columns\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "# 4. Create the full end-to-end pipeline including the model\n",
        "# This ensures raw data flows through preprocessing directly into the model\n",
        "model = LogisticRegression()\n",
        "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                           ('model', model)\n",
        "                          ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Training and Evaluation (Logistic Regression)\n",
        "\n",
        "Finally, we split the data into training and testing sets. We fit the entire pipeline on the training data and evaluate its performance on the unseen test data using various classification metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.84\n",
            "Precision: 0.83\n",
            "Recall: 0.98\n",
            "F1 Score: 0.90\n",
            "AUC: 0.83\n"
          ]
        }
      ],
      "source": [
        "# Split data: 80% for training, 20% for testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "# Train the pipeline\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Generate predictions\n",
        "preds = pipeline.predict(X_test)\n",
        "\n",
        "# Calculate classification metrics\n",
        "accuracy = accuracy_score(y_test, preds)\n",
        "precision = precision_score(y_test, preds)\n",
        "recall = recall_score(y_test, preds)\n",
        "f1 = f1_score(y_test, preds)\n",
        "\n",
        "# Calculate ROC curve and AUC score\n",
        "# Note: We use predict_proba for ROC/AUC to get probability scores instead of class labels\n",
        "fpr, tpr, thresholds = roc_curve(y_test, pipeline.predict_proba(X_test)[:,1])\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Output results\n",
        "print(f'Accuracy: {accuracy:.2f}')\n",
        "print(f'Precision: {precision:.2f}')\n",
        "print(f'Recall: {recall:.2f}')\n",
        "print(f'F1 Score: {f1:.2f}')\n",
        "print(f\"AUC: {roc_auc:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Alternative Model (KNN with MinMaxScaler)\n",
        "\n",
        "Now, we recreate the workflow but use min-max scaling for numerical features and KNN classifier for the model. This allows us to compare different preprocessing and modeling approaches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Redefine numerical pipeline with Min-Max Scaling\n",
        "# Min-Max scaling scales data to a fixed range [0, 1], which preserves the shape of the original distribution\n",
        "numerical_transformer_knn = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')), # Fill missing values\n",
        "    ('scaler', MinMaxScaler())                     # Scale to range [0, 1]\n",
        "])\n",
        "\n",
        "# 2. Update the ColumnTransformer\n",
        "# We reuse the 'categorical_transformer' defined in the previous section (Imputer + OneHotEncoder)\n",
        "preprocessor_knn = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer_knn, numerical_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "# 3. Define the new model: K-Nearest Neighbors\n",
        "model_knn = KNeighborsClassifier()\n",
        "\n",
        "# 4. Create the new pipeline\n",
        "pipeline_knn = Pipeline(steps=[('preprocessor', preprocessor_knn),\n",
        "                               ('model', model_knn)\n",
        "                              ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Training and Evaluation**\n",
        "\n",
        "We fit this new pipeline to the same training data used previously and evaluate its performance. This allows for a direct comparison between the Logistic Regression (StandardScaler) approach and this KNN (MinMaxScaler) approach."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.75\n",
            "Precision: 0.79\n",
            "Recall: 0.89\n",
            "F1 Score: 0.84\n",
            "AUC: 0.71\n"
          ]
        }
      ],
      "source": [
        "# Train the KNN pipeline\n",
        "pipeline_knn.fit(X_train, y_train)\n",
        "\n",
        "# Generate predictions\n",
        "preds_knn = pipeline_knn.predict(X_test)\n",
        "\n",
        "# Calculate classification metrics\n",
        "accuracy_knn = accuracy_score(y_test, preds_knn)\n",
        "precision_knn = precision_score(y_test, preds_knn)\n",
        "recall_knn = recall_score(y_test, preds_knn)\n",
        "f1_knn = f1_score(y_test, preds_knn)\n",
        "\n",
        "# Calculate ROC/AUC\n",
        "# Note: KNN supports predict_proba, which allows us to calculate AUC just like Logistic Regression\n",
        "fpr_knn, tpr_knn, thresholds_knn = roc_curve(y_test, pipeline_knn.predict_proba(X_test)[:,1])\n",
        "roc_auc_knn = auc(fpr_knn, tpr_knn)\n",
        "\n",
        "# Output results\n",
        "print(f'Accuracy: {accuracy_knn:.2f}')\n",
        "print(f'Precision: {precision_knn:.2f}')\n",
        "print(f'Recall: {recall_knn:.2f}')\n",
        "print(f'F1 Score: {f1_knn:.2f}')\n",
        "print(f\"AUC: {roc_auc_knn:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Comparison\n",
        "\n",
        "Let's compare the performance of both models side by side:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Comparison:\n",
            "   Metric  Logistic Regression  KNN (MinMaxScaler)\n",
            " Accuracy             0.837398            0.747967\n",
            "Precision             0.830189            0.792079\n",
            "   Recall             0.977778            0.888889\n",
            " F1 Score             0.897959            0.837696\n",
            "  ROC AUC             0.826263            0.713300\n"
          ]
        }
      ],
      "source": [
        "# Create a comparison DataFrame\n",
        "comparison = pd.DataFrame({\n",
        "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC'],\n",
        "    'Logistic Regression': [accuracy, precision, recall, f1, roc_auc],\n",
        "    'KNN (MinMaxScaler)': [accuracy_knn, precision_knn, recall_knn, f1_knn, roc_auc_knn]\n",
        "})\n",
        "\n",
        "print(\"Model Comparison:\")\n",
        "print(comparison.to_string(index=False))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
